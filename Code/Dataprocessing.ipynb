{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_avg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2b7e69c50038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mincrement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_simulations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mavg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_avg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_avg' is not defined"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "phis = [10, 18, 20, 25, 50, 5]\n",
    "max_n_avg = 36\n",
    "increment = 0.2\n",
    "num_simulations = 100\n",
    "n = len(p)\n",
    "\n",
    "n_avg_RG = np.arange(1, max_n_avg, increment)\n",
    "p = [avg/(N-1) for avg in n_avg_RG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = glob.glob('Results_HTML/*')\n",
    "x = [i.split('/')[1] for i in x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 17.00it/s]\n",
      "6it [00:00, 17.23it/s]\n",
      "6it [00:00, 34.35it/s]\n"
     ]
    }
   ],
   "source": [
    "RGD, RGU, SF = pd.DataFrame({\"Average Degree\": n_avg_RG}), pd.DataFrame({\"Average Degree\": n_avg_RG}), pd.DataFrame({\"Average Degree\": range(1,max_n_avg + 1)})\n",
    "\n",
    "for phi, fp in tqdm(zip(phis, sorted(glob.glob('./Results/RG_D*')))):\n",
    "    tmp = np.load(fp)\n",
    "    dims = (num_simulations, n)\n",
    "    graph = \"RGD\"\n",
    "    names = [\"0-5\", \"5-10\", \"10-15\", \"15-20\", \"0-10\", \"0-15\", \"0-20\", \"Normal\", \"95-100\"]\n",
    "    \n",
    "    s_05, s_10, s_15, s_20 =  np.reshape(tmp[:,0], dims),  np.reshape(tmp[:,1], dims),  np.reshape(tmp[:,2], dims),  np.reshape(tmp[:,3], dims)\n",
    "    s_010, s_015, s_020, s_n, s_95 = np.reshape(tmp[:,4], dims), np.reshape(tmp[:,5], dims),  np.reshape(tmp[:,6], dims),  np.reshape(tmp[:,7], dims),  np.reshape(tmp[:,8], dims)\n",
    "\n",
    "    t_05, t_10, t_15, t_20 = np.reshape(tmp[:,9], dims),  np.reshape(tmp[:,10], dims),  np.reshape(tmp[:,11], dims),  np.reshape(tmp[:,12], dims)\n",
    "    t_010, t_015, t_020, t_n, t_95 = np.reshape(tmp[:,13], dims), np.reshape(tmp[:,14], dims),  np.reshape(tmp[:,15], dims),  np.reshape(tmp[:,16], dims), np.reshape(tmp[:,17], dims)\n",
    "    \n",
    "    # Number of Nodes of Network Influenced\n",
    "    S_05, S_10, S_15, S_20 =  np.apply_along_axis(np.mean, 0, s_05), np.apply_along_axis(np.mean, 0, s_10), np.apply_along_axis(np.mean, 0, s_15), np.apply_along_axis(np.mean, 0, s_20)\n",
    "    S_010, S_015, S_020, S_n, S_95 = np.apply_along_axis(np.mean, 0, s_010), np.apply_along_axis(np.mean, 0, s_015), np.apply_along_axis(np.mean, 0, s_020), np.apply_along_axis(np.mean, 0, s_n), np.apply_along_axis(np.mean, 0, s_95)\n",
    "\n",
    "    # Proportion of Network Influenced\n",
    "    N_05, N_10, N_15, N_20 =  [x/N for x in S_05], [x/N for x in S_10], [x/N for x in S_15], [x/N for x in S_20]\n",
    "    N_010, N_015, N_020, N_n, N_95 = [x/N for x in S_010], [x/N for x in S_015], [x/N for x in S_020], [x/N for x in S_n], [x/N for x in S_95]\n",
    "\n",
    "    # Averaged Time of Influenced Nodes\n",
    "    T_05, T_10, T_15, T_20 =  np.apply_along_axis(np.mean, 0, t_05), np.apply_along_axis(np.mean, 0, t_10), np.apply_along_axis(np.mean, 0, t_15), np.apply_along_axis(np.mean, 0, t_20)\n",
    "    T_010, T_015, T_020, T_n, T_95 = np.apply_along_axis(np.mean, 0, t_010), np.apply_along_axis(np.mean, 0, t_015), np.apply_along_axis(np.mean, 0, t_020), np.apply_along_axis(np.mean, 0, t_n), np.apply_along_axis(np.mean, 0, t_95)\n",
    "\n",
    "    S = [S_05, S_10, S_15, S_20, S_010, S_015, S_020, S_n, S_95]\n",
    "    T = [T_05, T_10, T_15, T_20, T_010, T_015, T_020, T_n, T_95]\n",
    "    P = [N_05, N_10, N_15, N_20, N_010, N_015, N_020, N_n, N_95]\n",
    "    data = [list(x) for x in S+T+P]\n",
    "    \n",
    "    S_cols = [ \"{}_{}_{}_{}\".format(graph,name,\"S\",phi) for name in names]\n",
    "    T_cols = [ \"{}_{}_{}_{}\".format(graph,name,\"T\",phi) for name in names]\n",
    "    P_cols = [ \"{}_{}_{}_{}\".format(graph,name,\"N\",phi) for name in names]\n",
    "    RGD = pd.concat([RGD, pd.DataFrame(dict(zip(S_cols+T_cols+P_cols,data)))], axis=1, sort=False)\n",
    "\n",
    "\n",
    "\n",
    "for phi, fp in tqdm(zip(phis, sorted(glob.glob('./Results/RG_N*')))):\n",
    "    tmp = np.load(fp)\n",
    "    dims = (num_simulations, n)\n",
    "    graph = \"RGU\"\n",
    "    names = [\"0-5\", \"5-10\", \"10-15\", \"15-20\", \"0-10\", \"0-15\", \"0-20\", \"Normal\", \"95-100\"]\n",
    "\n",
    "    s_05, s_10, s_15, s_20 =  np.reshape(tmp[:,0], dims),  np.reshape(tmp[:,1], dims),  np.reshape(tmp[:,2], dims),  np.reshape(tmp[:,3], dims)\n",
    "    s_010, s_015, s_020, s_n, s_95 = np.reshape(tmp[:,4], dims), np.reshape(tmp[:,5], dims),  np.reshape(tmp[:,6], dims),  np.reshape(tmp[:,7], dims),  np.reshape(tmp[:,8], dims)\n",
    "\n",
    "    t_05, t_10, t_15, t_20 = np.reshape(tmp[:,9], dims),  np.reshape(tmp[:,10], dims),  np.reshape(tmp[:,11], dims),  np.reshape(tmp[:,12], dims)\n",
    "    t_010, t_015, t_020, t_n, t_95 = np.reshape(tmp[:,13], dims), np.reshape(tmp[:,14], dims),  np.reshape(tmp[:,15], dims),  np.reshape(tmp[:,16], dims), np.reshape(tmp[:,17], dims)\n",
    "    \n",
    "    # Number of Nodes of Network Influenced\n",
    "    S_05, S_10, S_15, S_20 =  np.apply_along_axis(np.mean, 0, s_05), np.apply_along_axis(np.mean, 0, s_10), np.apply_along_axis(np.mean, 0, s_15), np.apply_along_axis(np.mean, 0, s_20)\n",
    "    S_010, S_015, S_020, S_n, S_95 = np.apply_along_axis(np.mean, 0, s_010), np.apply_along_axis(np.mean, 0, s_015), np.apply_along_axis(np.mean, 0, s_020), np.apply_along_axis(np.mean, 0, s_n), np.apply_along_axis(np.mean, 0, s_95)\n",
    "\n",
    "    # Proportion of Network Influenced\n",
    "    N_05, N_10, N_15, N_20 =  [x/N for x in S_05], [x/N for x in S_10], [x/N for x in S_15], [x/N for x in S_20]\n",
    "    N_010, N_015, N_020, N_n, N_95 = [x/N for x in S_010], [x/N for x in S_015], [x/N for x in S_020], [x/N for x in S_n], [x/N for x in S_95]\n",
    "\n",
    "    # Averaged Time of Influenced Nodes\n",
    "    T_05, T_10, T_15, T_20 =  np.apply_along_axis(np.mean, 0, t_05), np.apply_along_axis(np.mean, 0, t_10), np.apply_along_axis(np.mean, 0, t_15), np.apply_along_axis(np.mean, 0, t_20)\n",
    "    T_010, T_015, T_020, T_n, T_95 = np.apply_along_axis(np.mean, 0, t_010), np.apply_along_axis(np.mean, 0, t_015), np.apply_along_axis(np.mean, 0, t_020), np.apply_along_axis(np.mean, 0, t_n), np.apply_along_axis(np.mean, 0, t_95)\n",
    "\n",
    "    S = [S_05, S_10, S_15, S_20, S_010, S_015, S_020, S_n, S_95]\n",
    "    T = [T_05, T_10, T_15, T_20, T_010, T_015, T_020, T_n, T_95]\n",
    "    P = [N_05, N_10, N_15, N_20, N_010, N_015, N_020, N_n, N_95]\n",
    "    data = [list(x) for x in S+T+P]\n",
    "    \n",
    "    S_cols = [ \"{}_{}_{}_{}\".format(graph,name,\"S\",phi) for name in names]\n",
    "    T_cols = [ \"{}_{}_{}_{}\".format(graph,name,\"T\",phi) for name in names]\n",
    "    P_cols = [ \"{}_{}_{}_{}\".format(graph,name,\"N\",phi) for name in names]\n",
    "    RGU = pd.concat([RGU,pd.DataFrame(dict(zip(S_cols+T_cols+P_cols,data)))], axis=1, sort=False)\n",
    " \n",
    "   \n",
    "for phi, fp in tqdm(zip(phis, sorted(glob.glob('./Results/SF*')))):\n",
    "    tmp = np.load(fp)\n",
    "    dims = (num_simulations, max_n_avg)\n",
    "    graph = \"SF\"\n",
    "    names = [\"0-5\", \"5-10\", \"10-15\", \"15-20\", \"0-10\", \"0-15\", \"0-20\", \"Normal\", \"95-100\"]\n",
    "    \n",
    "    s_05, s_10, s_15, s_20 =  np.reshape(tmp[:,0], dims),  np.reshape(tmp[:,1], dims),  np.reshape(tmp[:,2], dims),  np.reshape(tmp[:,3], dims)\n",
    "    s_010, s_015, s_020, s_n, s_95 = np.reshape(tmp[:,4], dims), np.reshape(tmp[:,5], dims),  np.reshape(tmp[:,6], dims),  np.reshape(tmp[:,7], dims),  np.reshape(tmp[:,8], dims)\n",
    "\n",
    "    t_05, t_10, t_15, t_20 = np.reshape(tmp[:,9], dims),  np.reshape(tmp[:,10], dims),  np.reshape(tmp[:,11], dims),  np.reshape(tmp[:,12], dims)\n",
    "    t_010, t_015, t_020, t_n, t_95 = np.reshape(tmp[:,13], dims), np.reshape(tmp[:,14], dims),  np.reshape(tmp[:,15], dims),  np.reshape(tmp[:,16], dims), np.reshape(tmp[:,17], dims)\n",
    "    \n",
    "    # Number of Nodes of Network Influenced\n",
    "    S_05, S_10, S_15, S_20 =  np.apply_along_axis(np.mean, 0, s_05), np.apply_along_axis(np.mean, 0, s_10), np.apply_along_axis(np.mean, 0, s_15), np.apply_along_axis(np.mean, 0, s_20)\n",
    "    S_010, S_015, S_020, S_n, S_95 = np.apply_along_axis(np.mean, 0, s_010), np.apply_along_axis(np.mean, 0, s_015), np.apply_along_axis(np.mean, 0, s_020), np.apply_along_axis(np.mean, 0, s_n), np.apply_along_axis(np.mean, 0, s_95)\n",
    "\n",
    "    # Proportion of Network Influenced\n",
    "    N_05, N_10, N_15, N_20 =  [x/N for x in S_05], [x/N for x in S_10], [x/N for x in S_15], [x/N for x in S_20]\n",
    "    N_010, N_015, N_020, N_n, N_95 = [x/N for x in S_010], [x/N for x in S_015], [x/N for x in S_020], [x/N for x in S_n], [x/N for x in S_95]\n",
    "\n",
    "    # Averaged Time of Influenced Nodes\n",
    "    T_05, T_10, T_15, T_20 =  np.apply_along_axis(np.mean, 0, t_05), np.apply_along_axis(np.mean, 0, t_10), np.apply_along_axis(np.mean, 0, t_15), np.apply_along_axis(np.mean, 0, t_20)\n",
    "    T_010, T_015, T_020, T_n, T_95 = np.apply_along_axis(np.mean, 0, t_010), np.apply_along_axis(np.mean, 0, t_015), np.apply_along_axis(np.mean, 0, t_020), np.apply_along_axis(np.mean, 0, t_n), np.apply_along_axis(np.mean, 0, t_95)\n",
    "\n",
    "    S = [S_05, S_10, S_15, S_20, S_010, S_015, S_020, S_n, S_95]\n",
    "    T = [T_05, T_10, T_15, T_20, T_010, T_015, T_020, T_n, T_95]\n",
    "    P = [N_05, N_10, N_15, N_20, N_010, N_015, N_020, N_n, N_95]\n",
    "    data = [list(x) for x in S+T+P]\n",
    "    \n",
    "    S_cols = [ \"{}_{}_{}_{}\".format(graph,name,\"S\",phi) for name in names]\n",
    "    T_cols = [ \"{}_{}_{}_{}\".format(graph,name,\"T\",phi) for name in names]\n",
    "    P_cols = [ \"{}_{}_{}_{}\".format(graph,name,\"N\",phi) for name in names]\n",
    "    SF = pd.concat([SF,pd.DataFrame(dict(zip((S_cols + T_cols+P_cols),data)))], axis=1, sort=False)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGD.to_csv(\"./Results/RGD.csv\")\n",
    "RGU.to_csv(\"./Results/RGu.csv\")\n",
    "SF.to_csv(\"./Results/SF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory= \"./Results\"\n",
    "datasets = [\"advogato\", \"facebook\"]\n",
    "dataset_N = [6551, 4039]\n",
    "file_paths = [os.path.join(directory, fn) + \".npy\" for fn in datasets]\n",
    "phis = sorted(phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 132.26it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Threshold Value\": phis})\n",
    "\n",
    "for dataset, fp, N in tqdm(zip(datasets, file_paths, dataset_N)):\n",
    "    tmp = np.load(fp)\n",
    "    names = [\"0-5\", \"5-10\", \"10-15\", \"15-20\", \"0-10\", \"0-15\", \"0-20\", \"Normal\", \"95-100\"]\n",
    "    \n",
    "    # Number of Nodes of Network Influenced\n",
    "    S_05, S_10, S_15, S_20 =   tmp[:,0], tmp[:,1], tmp[:,2], tmp[:,3]\n",
    "    S_010, S_015, S_020, S_n, S_95 =  tmp[:,4], tmp[:,5], tmp[:,6], tmp[:,7], tmp[:,8]\n",
    "\n",
    "    # Averaged Time of Influenced Nodes\n",
    "    T_05, T_10, T_15, T_20 =  tmp[:,9], tmp[:,10], tmp[:,11], tmp[:,12]\n",
    "    T_010, T_015, T_020, T_n, T_95 =  tmp[:,13], tmp[:,14], tmp[:,15], tmp[:,16], tmp[:,17]\n",
    "\n",
    "    # Proportion of Network Influenced\n",
    "    N_05, N_10, N_15, N_20 =  [x/N for x in S_05], [x/N for x in S_10], [x/N for x in S_15], [x/N for x in S_20]\n",
    "    N_010, N_015, N_020, N_n, N_95 = [x/N for x in S_010], [x/N for x in S_015], [x/N for x in S_020], [x/N for x in S_n], [x/N for x in S_95]\n",
    "\n",
    "\n",
    "    S = [S_05, S_10, S_15, S_20, S_010, S_015, S_020, S_n, S_95]\n",
    "    T = [T_05, T_10, T_15, T_20, T_010, T_015, T_020, T_n, T_95]\n",
    "    P = [N_05, N_10, N_15, N_20, N_010, N_015, N_020, N_n, N_95]\n",
    "    data = [list(x) for x in S+T+P]\n",
    "\n",
    "    S_cols = [ \"{}_{}_S\".format(dataset,name) for name in names]\n",
    "    T_cols = [ \"{}_{}_T\".format(dataset,name) for name in names]\n",
    "    P_cols = [ \"{}_{}_N\".format(dataset,name) for name in names]\n",
    "    df = pd.concat([df,pd.DataFrame(dict(zip((S_cols + T_cols+P_cols),data)))], axis=1, sort=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./Results/Real_Data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
