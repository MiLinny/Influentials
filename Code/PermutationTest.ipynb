{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello this has loaded\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "from SimulationHelper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "p = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.erdos_renyi_graph(N,p)\n",
    "set_influence(G, 0)\n",
    "set_time(G, 0)\n",
    "\n",
    "## Retrieve influential nodes - top q% and non-influential nodes\n",
    "degree_ordered_nodes = sorted(list(G.nodes()), key=lambda x: G.degree(x), reverse=True)\n",
    "influential_nodes_5   = degree_ordered_nodes[:int(0.05*N)]\n",
    "influential_nodes_10  = degree_ordered_nodes[int(0.05*N):int(0.1*N)]\n",
    "influential_nodes_15 = degree_ordered_nodes[int(0.1*N):int(0.15*N)]\n",
    "influential_nodes_20 = degree_ordered_nodes[int(0.15*N):int(0.2*N)]\n",
    "bottom_nodes = degree_ordered_nodes[int(0.9*N):]\n",
    "\n",
    "average = p * (N-1)\n",
    "lower, upper = int(np.floor(average)), int(np.ceil(average))\n",
    "normal_nodes = [x for x in G.nodes() if lower <= G.degree(x) <= upper ]\n",
    "\n",
    "influential_S_5, influential_S_10, influential_S_15, influential_S_20 = [], [], [], []\n",
    "influential_t_5, influential_t_10, influential_t_15, influential_t_20 = [], [], [], []\n",
    "normal_S, bottom_S = [], []\n",
    "normal_t, bottom_t = [], []\n",
    "\n",
    "## Calculate the number of influenced nodes (S) and expected time of influenced nodes\n",
    "## for each influential node\n",
    "\n",
    "## Generate pairs for influential nodes 0-5 and groups of 4 for influential_nodes_10\n",
    "influential_nodes_5_pairs = []\n",
    "influential_nodes_10_pairs = []\n",
    "influential_nodes_15_pairs = []\n",
    "influential_nodes_20_pairs = []\n",
    "normal_nodes_pairs = []\n",
    "bottom_nodes_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.71 ms, sys: 380 µs, total: 2.09 ms\n",
      "Wall time: 2.04 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in np.arange(len(influential_nodes_5)):\n",
    "    for j in np.arange(i, len(influential_nodes_5)):\n",
    "        influential_nodes_5_pairs.append([influential_nodes_5[i], influential_nodes_5[j]])\n",
    "for i in np.arange(len(influential_nodes_10)):\n",
    "    for j in np.arange(i, len(influential_nodes_10)):\n",
    "        influential_nodes_10_pairs.append([influential_nodes_10[i], influential_nodes_10[j]])            \n",
    "for i in np.arange(len(influential_nodes_15)):\n",
    "    for j in np.arange(i, len(influential_nodes_15)):\n",
    "        influential_nodes_15_pairs.append([influential_nodes_15[i], influential_nodes_15[j]])\n",
    "for i in np.arange(len(influential_nodes_20)):\n",
    "    for j in np.arange(i, len(influential_nodes_20)):\n",
    "        influential_nodes_20_pairs.append([influential_nodes_20[i], influential_nodes_20[j]])          \n",
    "for i in np.arange(len(normal_nodes)):\n",
    "    for j in np.arange(i, len(normal_nodes)):\n",
    "        normal_nodes_pairs.append([normal_nodes[i], normal_nodes[j]])\n",
    "for i in np.arange(len(bottom_nodes)):\n",
    "    for j in np.arange(i, len(bottom_nodes)):\n",
    "        bottom_nodes_pairs.append([bottom_nodes[i], bottom_nodes[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 78 µs, sys: 1e+03 ns, total: 79 µs\n",
      "Wall time: 80.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "influential_nodes_5_pairs = list(it.combinations(influential_nodes_5, 2))\n",
    "influential_nodes_10_pairs = list(it.combinations(influential_nodes_10, 2))\n",
    "influential_nodes_15_pairs = list(it.combinations(influential_nodes_15, 2))\n",
    "influential_nodes_20_pairs = list(it.combinations(influential_nodes_20, 2))\n",
    "normal_nodes_pairs = list(it.combinations(normal_nodes, 2))\n",
    "bottom_nodes_pairs = list(it.combinations(bottom_nodes, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_RG_PC(N,p,phi=0.18):\n",
    "    '''\n",
    "        Simulation of Poisson/Binomial Random Graph\n",
    "        Returns the average size of influenced nodes and average expected \n",
    "        time to be influenced from \n",
    "            - influential nodes\n",
    "            - normal nodes\n",
    "    '''\n",
    "    G = nx.erdos_renyi_graph(N,p)\n",
    "    set_influence(G, 0)\n",
    "    set_time(G, 0)\n",
    "    \n",
    "    ## Retrieve influential nodes - top q% and non-influential nodes\n",
    "    degree_ordered_nodes = sorted(list(G.nodes()), key=lambda x: G.degree(x), reverse=True)\n",
    "    influential_nodes_5   = degree_ordered_nodes[:int(0.05*N)]\n",
    "    influential_nodes_10  = degree_ordered_nodes[int(0.05*N):int(0.1*N)]\n",
    "    influential_nodes_15 = degree_ordered_nodes[int(0.1*N):int(0.15*N)]\n",
    "    influential_nodes_20 = degree_ordered_nodes[int(0.15*N):int(0.2*N)]\n",
    "    bottom_nodes = degree_ordered_nodes[int(0.9*N):]\n",
    "        \n",
    "    average = p * (N-1)\n",
    "    lower, upper = int(np.floor(average)), int(np.ceil(average))\n",
    "    normal_nodes = [x for x in G.nodes() if lower <= G.degree(x) <= upper ]\n",
    "\n",
    "    influential_S_5, influential_S_10, influential_S_15, influential_S_20 = [], [], [], []\n",
    "    influential_t_5, influential_t_10, influential_t_15, influential_t_20 = [], [], [], []\n",
    "    normal_S, bottom_S = [], []\n",
    "    normal_t, bottom_t = [], []\n",
    "    \n",
    "    ## Calculate the number of influenced nodes (S) and expected time of influenced nodes\n",
    "    ## for each influential node\n",
    "    \n",
    "    ## Generate pairs for influential nodes 0-5 and groups of 4 for influential_nodes_10\n",
    "    influential_nodes_5_pairs = []\n",
    "    influential_nodes_10_pairs = []\n",
    "    influential_nodes_15_pairs = []\n",
    "    influential_nodes_20_pairs = []\n",
    "    normal_nodes_pairs = []\n",
    "    bottom_nodes_pairs = []\n",
    "    #influential_nodes_10_groups = []\n",
    "    \n",
    "    for i in np.arange(len(influential_nodes_5)):\n",
    "        for j in np.arange(i, len(influential_nodes_5)):\n",
    "            influential_nodes_5_pairs.append([influential_nodes_5[i], influential_nodes_5[j]])\n",
    "    for i in np.arange(len(influential_nodes_10)):\n",
    "        for j in np.arange(i, len(influential_nodes_10)):\n",
    "            influential_nodes_10_pairs.append([influential_nodes_10[i], influential_nodes_10[j]])            \n",
    "    for i in np.arange(len(influential_nodes_15)):\n",
    "        for j in np.arange(i, len(influential_nodes_15)):\n",
    "            influential_nodes_15_pairs.append([influential_nodes_15[i], influential_nodes_15[j]])\n",
    "    for i in np.arange(len(influential_nodes_20)):\n",
    "        for j in np.arange(i, len(influential_nodes_20)):\n",
    "            influential_nodes_20_pairs.append([influential_nodes_20[i], influential_nodes_20[j]])          \n",
    "    for i in np.arange(len(normal_nodes)):\n",
    "        for j in np.arange(i, len(normal_nodes)):\n",
    "            normal_nodes_pairs.append([normal_nodes[i], normal_nodes[j]])\n",
    "    for i in np.arange(len(bottom_nodes)):\n",
    "        for j in np.arange(i, len(bottom_nodes)):\n",
    "            bottom_nodes_pairs.append([bottom_nodes[i], bottom_nodes[j]])\n",
    "    \n",
    "    #for node in influential_nodes_10:\n",
    "    #    inf_nod_10 = influential_nodes_10.copy()\n",
    "    #    inf_nod_10.remove(node)\n",
    "    #    influential_nodes_10_groups.append(inf_nod_10)\n",
    "    \n",
    "    \n",
    "    for node_list in influential_nodes_5_pairs:\n",
    "        S, t = simulate_spread_wrapper(G, node_list, phi)\n",
    "        influential_S_5.append(S)\n",
    "        influential_t_5.append(t)    \n",
    "    #for node_list in influential_nodes_10_groups:\n",
    "    #    S, t = simulate_spread_wrapper(G, node_list, phi)\n",
    "    #    influential_S_10.append(S)\n",
    "    #    influential_t_10.append(t)\n",
    "    for node_list in influential_nodes_10_pairs:\n",
    "        S, t = simulate_spread_wrapper(G, node_list, phi)\n",
    "        influential_S_10.append(S)\n",
    "        influential_t_10.append(t)\n",
    "    for node_list in influential_nodes_15_pairs:\n",
    "        S, t = simulate_spread_wrapper(G, node_list, phi)\n",
    "        influential_S_15.append(S)\n",
    "        influential_t_15.append(t)\n",
    "    for node_list in influential_nodes_20_pairs:\n",
    "        S, t = simulate_spread_wrapper(G, node_list, phi)\n",
    "        influential_S_20.append(S)\n",
    "        influential_t_20.append(t)\n",
    "    for node_list in normal_nodes_pairs:\n",
    "        S, t = simulate_spread_wrapper(G, node_list, phi)\n",
    "        normal_S.append(S)\n",
    "        normal_t.append(t)\n",
    "    for node_list in bottom_nodes_pairs:\n",
    "        S, t = simulate_spread_wrapper(G, node_list, phi)\n",
    "        bottom_S.append(S)\n",
    "        bottom_t.append(t)\n",
    "        \n",
    "   ## for node in influential_nodes_20:\n",
    "   ##     S, t = simulate_spread(G, node, phi)\n",
    "   ##     influential_S_20.append(S)\n",
    "   ##     influential_t_20.append(t)\n",
    "   ## for node in bottom_nodes:\n",
    "   ##     S, t = simulate_spread(G, node, phi)\n",
    "   ##     bottom_S.append(S)\n",
    "   ##     bottom_t.append(t)\n",
    "   ## for node in normal_nodes:\n",
    "   ##     S, t = simulate_spread(G, node, phi)\n",
    "   ##     normal_S.append(S)\n",
    "   ##     normal_t.append(t)\n",
    "    \n",
    "    return [np.mean(influential_S_5), np.mean(influential_S_10), np.mean(influential_S_15), np.mean(influential_S_20), np.mean(influential_S_5 + influential_S_10), np.mean(influential_S_5 + influential_S_10 + influential_S_15), np.mean(influential_S_5 + influential_S_10 + influential_S_15 + influential_S_20), np.mean(normal_S), np.mean(bottom_S),\n",
    "            np.mean(influential_t_5), np.mean(influential_t_10), np.mean(influential_t_15), np.mean(influential_t_20), np.mean(influential_t_5 + influential_t_10), np.mean(influential_t_5 + influential_t_10 + influential_t_15), np.mean(influential_t_5 + influential_t_10 + influential_t_15 + influential_t_20), np.mean(normal_t), np.mean(bottom_t)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
